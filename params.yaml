# params.yaml
# This key defines the default experiment to run.
experiment_profile: low_resource

# Each block below defines a full set of Spark configurations for an experiment.
# NOTE: The keys are now the full, official Spark property names.
spark_configs:
  baseline:
    spark.driver.memory: 2g
    spark.executor.cores: 1
    spark.sql.shuffle.partitions: 10

  low_resource:
    spark.driver.memory: 1g
    spark.executor.cores: 1
    spark.sql.shuffle.partitions: 5

  more_cores:
    spark.driver.memory: 2g
    spark.executor.cores: 2
    spark.sql.shuffle.partitions: 20

  high_parallelism:
    spark.driver.memory: 4g
    spark.executor.cores: 4
    spark.sql.shuffle.partitions: 50

